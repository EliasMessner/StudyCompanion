{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.text_splitter import NLTKTextSplitter, CharacterTextSplitter\n",
    "import pinecone\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"D:/Studium/BA-repo/dp-sent-analysis-twitter/thesis.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split PDF into parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Da diese Algorithmen nicht mit rohem Text arbeiten können, muss dieser in\n",
      "numerische Werte oder Vektoren umgewandelt werden [15].\n",
      "\n",
      "Dafür wird er in ein BoW-Modell\n",
      "umgewandelt.\n",
      "\n",
      "Ein BoW-Modell wird genutzt, um Features aus dem Rohtext zu gewinnen.\n",
      "\n",
      "Um dieses Modell\n",
      "generieren zu können, sind Informationen über das gegebene Vokabular, also die vorkommenden\n",
      "Wörter, und über die Metrik, welche das Vorkommen dieser Wörter bewertet, notwendig.\n",
      "\n",
      "Dabei\n",
      "wird die Reihenfolge der Dokumente im Korpus vernachlässigt.\n",
      "\n",
      "[15]\n",
      "AufdereinenSeitesindeinzelneWörteralskleinstelinguistischeEinheitgeeignet,denFeatureraum\n",
      "darzustellen.\n",
      "\n",
      "Jedoch bleibt hierbei der Kontext, der durch die Aneinanderreihung dieser Wörter\n",
      "entsteht, unbeachtet.\n",
      "\n",
      "[8]\n",
      "Für ein Dokument werden nun alle vorkommenden Wörter bestimmt, welche das Vokabular de-\n",
      "finieren.\n",
      "\n",
      "Mit jedem betrachteten Dokument wird dieses erweitert.\n",
      "\n",
      "Ist das Vokabular vollständig,\n",
      "können alle Dokumente in Wortvektoren umgewandelt werden.\n"
     ]
    }
   ],
   "source": [
    "# text in sätze splitten (chunk size anpassbar, also wie viele sätze (nach chars))\n",
    "text_splitter = NLTKTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "document_split = text_splitter.split_documents(document)\n",
    "\n",
    "texts = [t.page_content for t in document_split]\n",
    "\n",
    "print(texts[25])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
    "from array import array\n",
    "from pinecone import Vector\n",
    "\n",
    "# initialize pinecone\n",
    "pinecone.init(\n",
    "    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n",
    "    environment=PINECONE_API_ENV\n",
    ")\n",
    "index_name = \"se4ai\"\n",
    "\n",
    "#specify embedding model\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "#upload embeddings into Pinecone vector store\n",
    "docsearch = Pinecone.from_documents(document_split, embeddings, index_name=index_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.\n",
      "\n",
      "Sentiment Analyse auf dem Datensatz\n",
      "Abbildung 4.1.: „LRrangetest“[45]angewandtaufunserenDatensatz:KostenwertinAbhängigkeit\n",
      "von der Lernrate.\n",
      "\n",
      "angibt, welcher Anteil korrekt klassifiziert wurde, der Kostenwert jedoch zusätzlich quantifiziert,\n",
      "wie sicher das Modell bei der Klassifizierung ist.\n",
      "\n",
      "Zusätzlich wird Smiths Idee zum zyklischen Verändern der Lernrate angewandt [45].\n",
      "\n",
      "Aller 4 Epo-\n",
      "chen wird ein Zyklus durchlaufen, bei dem die Lernrate von 10−4auf10−2steigt, bevor sie wieder\n",
      "auf10−4sinkt.\n",
      "\n",
      "Der Vorteil liegt darin, dass Sattelpunkte der Kostenfunktion durch zeitweises Er-\n",
      "höhen der Lernrate übersprungen werden können [45].\n",
      "\n",
      "Eine zu kleine Lernrate könnte nicht den\n",
      "nötigen Gradienten hervorbringen, um diesem Sattelpunkt zu entkommen.\n",
      "\n",
      "Hier wird das Training\n",
      "ebenfalls nach 7 Epochen ohne verbesserte Validierungskosten beendet.\n",
      "\n",
      "In Abb.\n"
     ]
    }
   ],
   "source": [
    "query = \"Welche Lernrate wurde verwendet?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "print(docs[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "542bad02869b321a73a7da5260ee6f5456fa9b86f73b69a004054f84e7b0338b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
